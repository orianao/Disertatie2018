{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "7e5b5802-f908-ac4e-87f6-89d37edfeeed",
    "_uuid": "f188b9c651ee0a64a3c6b1fb783d54e9e39b435f"
   },
   "source": [
    "A Keras CNN to classify set A. As many files lacked classification, and I wasn't happy with the classification on the other files, I re-labelled all files and obtained decent results. I learnt a lot in the process, not least about the difficulties in using human perception as ground truth, and how data quality is more important than the algorithm. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "a4164221-e4cd-a4b6-3105-728b92b79402",
    "_uuid": "6998ef0f5e4451734b36f37e12ee4d86d369c917"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.io import wavfile\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "from scipy.signal import decimate\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "_cell_guid": "6a96e14a-3452-34f1-f32d-8a29bf4688cd",
    "_uuid": "0a53e3e6c25574c85fee730a44e4e1ab82456b6e"
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Conv1D, MaxPool1D, GlobalAvgPool1D, Dropout, BatchNormalization, Dense\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import ModelCheckpoint, LearningRateScheduler, EarlyStopping\n",
    "from keras.utils import np_utils\n",
    "from keras.regularizers import l2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "_cell_guid": "f392b235-02db-97d1-c2d6-869f7689d8a6",
    "_uuid": "29ec19c2880fc5ba02c304a9a215336304d3636e"
   },
   "outputs": [],
   "source": [
    "INPUT_LIB = 'heartbeat-sounds/'\n",
    "SAMPLE_RATE = 44100\n",
    "CLASSES = ['artifact', 'normal', 'murmur']\n",
    "CODE_BOOK = {x:i for i,x in enumerate(CLASSES)}   \n",
    "NB_CLASSES = len(CLASSES)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "84fc95d4-fd22-0b18-b1ad-ad518fd1ecb6",
    "_uuid": "97dadfe9793ffe083e3dbca9ea6a91c65f65b3a6"
   },
   "source": [
    "## Load the data\n",
    "We will need to preprocess the unclassified data, as they are marked NaN and also have incorrect file names. For now, we will also make all time series have equal length. We will do all this by defining element wise functions, that we can pass to Pandas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "_cell_guid": "80517cc2-4829-6638-d9aa-3f32f3e48a70",
    "_uuid": "da93dd5ec0ca969e62b619f7086c7f7bbc636b3a"
   },
   "outputs": [],
   "source": [
    "def clean_filename(fname, string):   \n",
    "    file_name = fname.split('/')[1]\n",
    "    if file_name[:2] == '__':        \n",
    "        file_name = string + file_name\n",
    "    return file_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "_cell_guid": "3b8b23a9-99b7-5d2a-3866-500735fa07ca",
    "_uuid": "6273dcfde4b4a350b782d7cd272cdb99ba14e9fc"
   },
   "outputs": [],
   "source": [
    "def load_wav_file(name, path):\n",
    "    _, b = wavfile.read(path + name)\n",
    "    assert _ == SAMPLE_RATE\n",
    "    return b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "_cell_guid": "100cf856-64e3-678d-f4bb-96bb072b2d77",
    "_uuid": "81ef5312c60ea8a6f3882660976ee83f2dcd9d88"
   },
   "outputs": [],
   "source": [
    "def repeat_to_length(arr, length):\n",
    "    \"\"\"Repeats the numpy 1D array to given length, and makes datatype float\"\"\"\n",
    "    result = np.empty((length, ), dtype = 'float32')\n",
    "    l = len(arr)\n",
    "    pos = 0\n",
    "    while pos + l <= length:\n",
    "        result[pos:pos+l] = arr\n",
    "        pos += l\n",
    "    if pos < length:\n",
    "        result[pos:length] = arr[:length-pos]\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "_cell_guid": "9abd650c-ebc1-d5ae-9dbd-0417ffbea338",
    "_uuid": "2a92f263fac949fe40accbf9bd30ab1a10d37407"
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "File b'../input/set_a.csv' does not exist",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-b0218efb8160>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mINPUT_LIB\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'set_a.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'fname'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'fname'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclean_filename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstring\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'Aunlabelledtest'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'label'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfillna\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'unclassified'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'time_series'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'fname'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mload_wav_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mINPUT_LIB\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'set_a/'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'len_series'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'time_series'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, escapechar, comment, encoding, dialect, tupleize_cols, error_bad_lines, warn_bad_lines, skipfooter, skip_footer, doublequote, delim_whitespace, as_recarray, compact_ints, use_unsigned, low_memory, buffer_lines, memory_map, float_precision)\u001b[0m\n\u001b[1;32m    707\u001b[0m                     skip_blank_lines=skip_blank_lines)\n\u001b[1;32m    708\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 709\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    710\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    711\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    447\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    448\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 449\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    450\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    451\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    816\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'has_index_names'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'has_index_names'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    817\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 818\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    819\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    820\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1047\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'c'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1048\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'c'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1049\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1050\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1051\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'python'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m   1693\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'allow_leading_cols'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex_col\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1694\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1695\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1696\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1697\u001b[0m         \u001b[0;31m# XXX\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: File b'../input/set_a.csv' does not exist"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(INPUT_LIB + 'set_a.csv')\n",
    "df['fname'] = df['fname'].apply(clean_filename, string='Aunlabelledtest')\n",
    "df['label'].fillna('unclassified')\n",
    "df['time_series'] = df['fname'].apply(load_wav_file, path=INPUT_LIB + 'set_a/')    \n",
    "df['len_series'] = df['time_series'].apply(len)\n",
    "MAX_LEN = max(df['len_series'])\n",
    "df['time_series'] = df['time_series'].apply(repeat_to_length, length=MAX_LEN) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "3b7bfb65-be64-3c58-3cf3-cdac499de4a3",
    "_uuid": "ddc0a276938f8f8006af01caebd6367c9478ede9"
   },
   "source": [
    "## Convert data to numpy arrays\n",
    "Let's leave the unclassified files for validation, and make a training set of the others. We will zero-pad the time series at the end to make the all the same length, then collect all in 2D numpy arrays, that can be used for neural network training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "f2c182b6-d33f-a7e6-bf92-74fd546e1cd5",
    "_uuid": "5f224f7e611b358247ac308477785e1f3fb442c2"
   },
   "outputs": [],
   "source": [
    "x_data = np.stack(df['time_series'].values, axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "806b3e25-65e5-842a-3be5-72b75af27530",
    "_uuid": "25e7bd2648eb0ceb3e4ce775fd5e36aa31c77153"
   },
   "source": [
    "Now, as explained in another [notebook][1], I will not use the classification from new_info['target'] but instead my own labels, with three classes 0=artifact, 1=normal/extrahls, and 2=murmur.\n",
    "\n",
    "\n",
    "  [1]: https://www.kaggle.com/toregil/d/kinguistics/heartbeat-sounds/misclassified-files-in-set-a/editnb \"notebook\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "535b3480-0cbb-21cb-a7bb-6d687a4719b1",
    "_uuid": "e74e48cf670c65b3c773de7760ee8b2950d49f30"
   },
   "outputs": [],
   "source": [
    "new_labels =[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "             0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 1, 1, 1,\n",
    "             1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 2, 2, 2, 1, 1, 2, 1, \n",
    "             2, 2, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, \n",
    "             2, 2, 2, 2, 2, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 2, 1, 1, \n",
    "             1, 1, 1, 1, 1, 2, 2, 1, 1, 1, 1, 1, 2, 1, 0, 2, 2, 1, 1, 1, 1, 1, \n",
    "             0, 1, 0, 1, 1, 1, 2, 1, 0, 1, 1, 1, 1, 1, 2, 0, 0, 0, 0, 0, 0, 0, \n",
    "             1, 0, 0, 0, 0, 0, 0, 1, 0, 2, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]\n",
    "new_labels = np.array(new_labels, dtype='int')\n",
    "y_data = np_utils.to_categorical(new_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "dda2dd6d-d6a2-0db1-3a7c-18b3d2f65c7d",
    "_uuid": "fd5eecc035b3a17ae102900c0bbc670988556291"
   },
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test, train_filenames, test_filenames = \\\n",
    "    train_test_split(x_data, y_data, df['fname'].values, test_size=0.25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "56def171-ceb8-45af-dd85-baff3fea46e6",
    "_uuid": "08776edeca8d957ac5ba8d091c0e2b99bac682c1"
   },
   "source": [
    "We now downsample the data with what is in effect a very aggressive low pass filter. This is not needed for computational time, but it seems to improve generalization on this dataset. With more data, we should remove or reduce this step and instead add 2-5 extra convolution layers. The reason this works is probably that what you hear in the stethoscope is almost exclusively low frequency sounds, especially murmurs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "a267d4df-9ddd-f866-cdd4-cff2097218f7",
    "_uuid": "d7ab1c7d9eebf94bbcde4abe7dd1ce65e1e39611"
   },
   "outputs": [],
   "source": [
    "x_train = decimate(x_train, 8, axis=1, zero_phase=True)\n",
    "x_train = decimate(x_train, 8, axis=1, zero_phase=True)\n",
    "x_train = decimate(x_train, 4, axis=1, zero_phase=True)\n",
    "x_test = decimate(x_test, 8, axis=1, zero_phase=True)\n",
    "x_test = decimate(x_test, 8, axis=1, zero_phase=True)\n",
    "x_test = decimate(x_test, 4, axis=1, zero_phase=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "968b5f27-8fbe-77c6-2d9b-4bfd9481132e",
    "_uuid": "3d2f38ec54013969974d1967cb1898ed9b74adc3"
   },
   "outputs": [],
   "source": [
    "#Scale each observation to unit variance, it should already have mean close to zero.\n",
    "x_train = x_train / np.std(x_train, axis=1).reshape(-1,1)\n",
    "x_test = x_test / np.std(x_test, axis=1).reshape(-1,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "00e5e67f-4f2c-842b-4326-e28dba46a31b",
    "_uuid": "569f85f71f0a4e8f400944fd1947169026b842e5"
   },
   "source": [
    "Keras wants the data two have a channel dimension, analogous to RGB for image recognition. Let's add this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "717c27bf-4639-e607-b9ca-e23c58ab80c1",
    "_uuid": "dfe8b2103d6a4065545dced485ca4dacb98883f8"
   },
   "outputs": [],
   "source": [
    "x_train = x_train[:,:,np.newaxis]\n",
    "x_test = x_test[:,:,np.newaxis]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "e3f3cf03-ee0e-5db8-382b-ec69ce47c7c5",
    "_uuid": "a0d35442feedae39d469123c275723a60880e67c"
   },
   "source": [
    "##Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "75ca2f52-9bc3-2f65-15c6-279b31bc823c",
    "_uuid": "27de5f6ff2b3939f209bd81b2fdf60d74768f946"
   },
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Conv1D(filters=4, kernel_size=9, activation='relu',\n",
    "                input_shape = x_train.shape[1:],\n",
    "                kernel_regularizer = l2(0.025)))\n",
    "model.add(MaxPool1D(strides=4))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Conv1D(filters=4, kernel_size=9, activation='relu',\n",
    "                kernel_regularizer = l2(0.05)))\n",
    "model.add(MaxPool1D(strides=4))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Conv1D(filters=8, kernel_size=9, activation='relu',\n",
    "                 kernel_regularizer = l2(0.1)))\n",
    "model.add(MaxPool1D(strides=4))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Conv1D(filters=16, kernel_size=9, activation='relu'))\n",
    "model.add(MaxPool1D(strides=4))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Conv1D(filters=64, kernel_size=4, activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Conv1D(filters=32, kernel_size=1, activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.75))\n",
    "model.add(GlobalAvgPool1D())\n",
    "model.add(Dense(3, activation='softmax'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "f1551e66-8131-db8e-22f1-6f8300293adf",
    "_uuid": "4a3e0951297a4974bc7893c473da72183afc0f3b"
   },
   "source": [
    "This version of the net has 5.000 parameters, and it could easily overfit our dataset of 125 time series if we let it run too long."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "7d39e984-ed9c-c3c3-1941-418fd00421c8",
    "_uuid": "1a032e31c9d2ad220659bc116958bf3e3c807aed"
   },
   "outputs": [],
   "source": [
    "def batch_generator(x_train, y_train, batch_size):\n",
    "    \"\"\"\n",
    "    Rotates the time series randomly in time\n",
    "    \"\"\"\n",
    "    x_batch = np.empty((batch_size, x_train.shape[1], x_train.shape[2]), dtype='float32')\n",
    "    y_batch = np.empty((batch_size, y_train.shape[1]), dtype='float32')\n",
    "    full_idx = range(x_train.shape[0])\n",
    "    \n",
    "    while True:\n",
    "        batch_idx = np.random.choice(full_idx, batch_size)\n",
    "        x_batch = x_train[batch_idx]\n",
    "        y_batch = y_train[batch_idx]\n",
    "    \n",
    "        for i in range(batch_size):\n",
    "            sz = np.random.randint(x_batch.shape[1])\n",
    "            x_batch[i] = np.roll(x_batch[i], sz, axis = 0)\n",
    "     \n",
    "        yield x_batch, y_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "15bf1459-d424-cf33-150d-25b13bf78fe8",
    "_uuid": "96e7b9da206e7e453164ced65f463a7453a7f39d"
   },
   "outputs": [],
   "source": [
    "weight_saver = ModelCheckpoint('set_a_weights.h5', monitor='val_loss', \n",
    "                               save_best_only=True, save_weights_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "595b4563-28a1-b467-c078-d2e5f00dd426",
    "_uuid": "8dd816a580bca8216e3fa5d55b99a88249b20f8f"
   },
   "outputs": [],
   "source": [
    "model.compile(optimizer=Adam(1e-4), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "annealer = LearningRateScheduler(lambda x: 1e-3 * 0.8**x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "f5a1e08c-d013-e145-8037-a5381bc379ab",
    "_uuid": "dfd8e38b83c7d95ea4c1ad8c8d39e78bc1716f71"
   },
   "outputs": [],
   "source": [
    "hist = model.fit_generator(batch_generator(x_train, y_train, 8),\n",
    "                   epochs=30, steps_per_epoch=1000,\n",
    "                   validation_data=(x_test, y_test),\n",
    "                   callbacks=[weight_saver, annealer],\n",
    "                   verbose=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "37c61291-51d4-3441-2e11-8708bef2051c",
    "_uuid": "f5090778d3f81788f3ca0996b1251a23be0f3d60"
   },
   "source": [
    "Let's bring back the best weights, in case we overfitted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "d4767dbc-923a-0677-eb54-842bb3f077d3",
    "_uuid": "503dc6af346ea6c253f7c7194ecc1e2b02ad1162"
   },
   "outputs": [],
   "source": [
    "model.load_weights('set_a_weights.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "14b1b34a-d6c1-b015-9395-f582b6b1b5cc",
    "_uuid": "1b6696174e39c071771aa6f52b7eb06b3ef89432"
   },
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "d4172b5d-bc2d-9c01-2f7c-8956047a6596",
    "_uuid": "cbf8e40e3df6a305977a20fc264cb2fa2b1c18ba"
   },
   "outputs": [],
   "source": [
    "plt.plot(hist.history['loss'], color='b')\n",
    "plt.plot(hist.history['val_loss'], color='r')\n",
    "plt.show()\n",
    "plt.plot(hist.history['acc'], color='b')\n",
    "plt.plot(hist.history['val_acc'], color='r')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "85a84d48-2357-8f7f-1598-c27108ff539e",
    "_uuid": "b3e861326800008795a05b4485a62712a43acd3e"
   },
   "outputs": [],
   "source": [
    "y_hat = model.predict(x_test)\n",
    "np.set_printoptions(precision=2, suppress=True)\n",
    "for i in range(3):\n",
    "    plt.plot(y_hat[:,i], c='r')\n",
    "    plt.plot(y_test[:,i], c='b')\n",
    "    plt.show()\n",
    "    print(CLASSES[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "26f8f49d-79d2-7430-f04f-895598e9047d",
    "_uuid": "a17b4f94ca33c4a0a67f05c2f3097920bbe8227d"
   },
   "outputs": [],
   "source": [
    "y_pred = np.argmax(y_hat, axis=1)\n",
    "y_true = np.argmax(y_test, axis=1)\n",
    "for i in range(len(y_true)):\n",
    "    if y_pred[i] != y_true[i]:\n",
    "        print(\"File: {}, Pred: {}, True: {}\".format(\n",
    "            test_filenames[i],\n",
    "            CLASSES[y_pred[i]], CLASSES[y_true[i]]))\n",
    "        plt.plot(x_test[i])\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "0b44d7f7-9df9-eeed-7ec1-8a9cc38508ec",
    "_uuid": "0ddfc64eed12c71d46df332a7d16a86b1161a454"
   },
   "source": [
    "Not too bad. It found the artifacts and the mistakes were non-obvious ones.   If this dataset really comes from an iPhone app, it is truly impressive.\n",
    "\n",
    " I'd be really grateful if someone could double check my labelling, both on training and test set. Check the \"New labels for set A\" [notebook][1].\n",
    "\n",
    "\n",
    "  [1]: https://www.kaggle.com/toregil/d/kinguistics/heartbeat-sounds/new-labels-for-set-a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "7b7a673b-b8dc-52a8-4fd9-bb7c6c4c0681",
    "_uuid": "6c52fc7e1cf8983b5286b1b0b35c11ef7b7fe495"
   },
   "source": [
    "I'll be back with an analysis of set B."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "71120b26-32d4-ff9e-828b-5b082796b118",
    "_uuid": "02533a54890d5d9b558c08822ae36e0016b0981b"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "544aaf9e-fe32-51e9-fef0-b8daeaba9f80",
    "_uuid": "53623c789519d11022e650ada40410eedd04dec2"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "_change_revision": 0,
  "_is_fork": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
